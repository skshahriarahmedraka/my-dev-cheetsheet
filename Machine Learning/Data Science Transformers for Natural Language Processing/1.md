RNN

text generation

name entity Recognithon (NER)

text summarization 

zero shot classification

model and tokenization

transfer learning



pretrained CNN

TF-IDF

raw sensor data

coustom feature

ANN

linear regression

random forest

SVM



transfer learning: take parameters learned from one task  and use them in some capacity for a different task

fine-tuning: adjust parameters "slowly" and "carefully" a bit further to improve performance



greedy layer- wise pretraining



possible pretrained task

- causal language model (autoregressive LM)(GPT - like)

- masked language model (auto encoder LM) (BERT -like)



#### Next sentence prediction

- BERT uses 2 pre training tasks - maked LM and NSP

- NSP is not a language mod


