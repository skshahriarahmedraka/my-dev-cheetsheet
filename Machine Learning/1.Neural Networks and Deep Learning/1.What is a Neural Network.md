# What is a Neural Network

rectified linear unit (relu)

The **rectified linear activation function** or **ReLU** for short is a piecewise linear function that will output the input 
directly if it is positive, otherwise, it will output zero. It has 
become the default activation function for many types of neural networks
 because a model that uses it is easier to train and often achieves 
better performance.

A feature vector is **a vector containing multiple elements about an object**.
 Putting feature vectors for objects together can make up a feature 
space. The features may represent, as a whole, one mere pixel or an 
entire image. The granularity depends on what someone is trying to learn
 or represent about the object.

computational graph  are a visula representation of expressing  and evaluating mathmetical equaltion . the  nodes are dataflow in graph correspond to mathmetical operator and variables .

keras  offers a python user friendly front end while maintain strong  computation powser by using  a low level api like tensorflow, pytorch etc. which use computation graph as backend 

- allows for abstraction of complex problem while specifying control flow 

- easier to implement distributed computation 

- useful for calculating derived by using back propagation

- allow parallelism which means that two operation can run simultaneously

nural networks are algorithms fashioned after the human brain contain multiple layers each layer contains node called neuron which performs a mathmetical operation they break down complex problem  into simple  mathmeticlal operation . they break down complex problems into simple operations 

Sequential models are linear stacks  of layers where one layer lead to the next. It is simple and  easy to impliment and you just have to make sure that the previous layer is the input to the next layer
