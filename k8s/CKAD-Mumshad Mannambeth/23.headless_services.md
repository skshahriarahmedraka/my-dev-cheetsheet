

when we deploy a 3 replica set of mysql  there is a master node and 3 slave node 

 cluster ip : 10.96.0.10

url : mysql.default.svc.cluster.local



pod ip address can change , like 10.40.2.8

pod dns address can change  , like 10-40-2.default.pod.cluster.local



## headless service

- headless service is a normal service , but it doesn't have an ip on it's own

- doesnot perform any load balance

- it create dns entry for each pod 

- every pod get a dns entry
  
  - `podname.headless-servicename.namespace.svc.cluster-domain.example`

- exmaple 
  
  - ```
    mysql-0.mysql-h.default.svc.cluster.local
    mysql-1.mysql-h.default.svc.cluster.local
    mysql-2.mysql-h.default.svc.cluster.local
    ```
  
  - this dns entry will alwas point to mysql master pod



## Headless service

headless-service.yaml

```
apiVersion: v1
kind: Service 
metadata:
    name: mysql-h 
spec:
    ports:
        - port: 3306
    selector:
        app: mysql
    clusterIP: None
```

pod-defination.yaml

```
apiVersion: v1 
kind: Pod 
metadata:
    name: myapp-pod 
    labels:   
        app: mysql
    spec:
        containers:
        - name: mysql 
          image: mysql
    subdomain: mysql-h
    hostname: mysql-pod
```

it create a dns record `mysql-pod.mysql-h.default.svc.cluster.local`

but it still doesn't create A record 



if we add subdomain in  deployment section it will assign same DNS  `mysql-pod.mysql-h.default.svc.cluster.local` 





### difference between stateful set and deployment

when using deployment we have to specify subdomain field 

In Stateful set we dont have to specify sub domain 

in stateful set we have to specify `serviceName: mysql-h`

 `statefulset-defination.yaml`

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
    name: mysql-deployment
    labels:
        app: mysql
spec: 
    serviceName: mysql-h   # mandatory
    replicas: 3
    matchLabels:
        app: mysql
    template:
        metadata:
            name: myapp-pod
            labels:
                app: mysql
        spec:
            containers:
            - name: mysql
              image: mysql
```



## Volume Claim Template

pvc-defination.yaml

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: data-volume
spec:
    accessModes:
        - ReadWriteOnece
    storageClassName: google-storage
    resources:
        requests:
            storage: 500Mi
```

statfulset-defination.yaml

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
    name: mysql
    labels:
        app: mysql
spec:
    replicas: 3
    selector:
        matchLabels:
            app: mysql
    template:
        meatadata:
            labels:
                app: mysql
        spec:
            containers:
            - name: mysql
              image: mysql
              volumeMounts:
              - mountPath: /var/lib/mysql
                name: data-volume
            volumes:
                - name: data-volume
                  persistentVolumeClaim:
                      claimName: data-volume
```





create a statefulset for volume claim teplates

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
    name: mysql
    labels:
        app: mysql
spec:
    replicas: 3
    selector:
        matchLabels:
            app: mysql
    template:
        meatadata:
            labels:
                app: mysql
        spec:
            containers:
            - name: mysql
              image: mysql
              volumeMounts:
              - mountPath: /var/lib/mysql
                name: data-volume
    volumeClaimTemplates
    - metadata:
           name: data-volume
      spec:
        accessModes:
            - ReadWriteOnece
      storageClassName: google-storage
      resources:
           requests:
                storage: 500Mi
        
```

`sc-defination.yaml`

```
apiVersion: storage.k8s.io/v1
kind: StorageClass 
metadata:
    name: goolge-storage
provisioner: kubernetes.io/gce-pd
```

here one by one persistent volume and percistent volume claim is created with it's storage

stateful set doesn't delete persistent volume claim autometically if pod fails . it re attached it with the previous persistent storage












